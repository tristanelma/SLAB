{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Auto Encoder\n",
    "\n",
    "By: Chengyi (Jeff) Chen\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this notebook, we will accomplish the following:\n",
    "1. Generate noisy images from this [repo](https://github.com/Belval/TextRecognitionDataGenerator) using the names of the currently active businesses in LA\n",
    "2. Implement a denoising auto encoder to clean / repair the augmented / noisy images so that we can feed the cleaned images into an OCR (Google's) for text recognition\n",
    "\n",
    "## Motivation\n",
    "\n",
    "- In order to identify the diversity demographic makeup of currently operating businesses in different parts of LA.\n",
    "\n",
    "## Data\n",
    "\n",
    "- To generate a list of relevant images like store names in LA, we will first pull the list of active businesses in LA from data.lacity.org and feed those names into our image generator, subsequently augmenting the simulated store sign images to be blocked by obstacles such as branches, trees, lamposts, etc... so that we have a representative training set that will \\be fed into our denoising autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Utilities\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import subprocess\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "\n",
    "# ML / Deep learning Utilities\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "from pylab import rcParams\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-paper')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Downloading Dataset\n",
    "__We will first grab the list of all active businesses in LA from [data.lacity.org](https://dev.socrata.com/foundry/data.lacity.org/ngkp-kqkn)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create a new folder\n",
    "def mkdir(path):\n",
    "    try: \n",
    "        os.makedirs(path)\n",
    "    except OSError:\n",
    "        if not os.path.isdir(path):\n",
    "            raise\n",
    "        else:\n",
    "            print(\"(%s) already exists\" % (path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data into data/lacity in the current directory\n",
    "# run !ls data/lacity to see the csv it downloaded\n",
    "!wget --directory-prefix=data/lacity/ -Nq https://data.lacity.org/api/views/6rrh-rzua/rows.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare directory to write results and file to read data from\n",
    "LACITY_DATA_LOC = 'data/lacity/rows.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's checkout the dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(LACITY_DATA_LOC)\n",
    "raw_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Seems like Business name is what we really need as it is the title that would likely appear on the storefront__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_df['BUSINESS NAME'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__There are duplicate Business names in the dataset, so let's remove them and store in a list__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting only the unique business names and removing the weird \"/C\"s\n",
    "business_names = [name[:-3] if name[-3:] == ' /C' or name[-2:] == '/C' else name for name in raw_df['BUSINESS NAME'].unique()]\n",
    "\n",
    "# TODO: Escaping the \"/\" in the names to prevent it \n",
    "# from messing up the name format later on\n",
    "'''\n",
    "    Use Regex to escape \"/\" because the TextRecognitionDataGeneretor can't escape it\n",
    "'''\n",
    "\n",
    "# How many active businesses in LA\n",
    "print('Number of active businesses in LA: %d' % len(business_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's write these names into a .txt file which we will use for the image generation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of .txt files that can be used \n",
    "# for the text image data generation\n",
    "DICT_LOC = './dicts/sp.txt'\n",
    "\n",
    "# Remove any old la_active_business_names.txt\n",
    "subprocess.call(['rm', '-rf', DICT_LOC])\n",
    "\n",
    "# Writing all the active LA business names\n",
    "# into the .txt file\n",
    "with open(DICT_LOC, 'w') as f:\n",
    "    for idx, name in enumerate(business_names):\n",
    "        f.write(\"%s\\n\" % name)\n",
    "        \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Generation\n",
    "__Now let's use the [TextRecognitionDataGenerator](https://github.com/Belval/TextRecognitionDataGenerator) to generate images of those active LA Business names. Firstly, we will create 8 images for each business name and store it in a folder for original images. Secondly, we will augment all of the images with random rotations, line streaks to cover parts of the image, etc... and store them in a folder for altered images which we will use to feed into the denoising auto encoder as the input. The original images will be treated as the true outputs that our denoised images will be compared to, determing the loss.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory of where our training / testing\n",
    "# dataset will be stored\n",
    "TRAIN_LOC = './data/training'\n",
    "TEST_LOC = './data/testing'\n",
    "\n",
    "# Remove any old data for training / testing\n",
    "subprocess.call(['rm', '-rf', TRAIN_LOC])\n",
    "subprocess.call(['rm', '-rf', TEST_LOC]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's generate the original images for training and test set - we would like a 80% training, 20% testing split for the data, so with 482,884 business names, and 8 images for each business name in the original image set, we would generate 3,863,072 images for our training set and 965,768 images for our testing set (P.S. Now I'm only creating 10,000 images to try out)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create new directories for the augmented images\n",
    "mkdir(TRAIN_LOC + '/original')\n",
    "mkdir(TEST_LOC + '/original')\n",
    "\n",
    "# Directory of the run.py file for TextRecognitionDataGenerator\n",
    "# & original images for training\n",
    "DATAGEN_LOC = '../TextRecognitionDataGenerator/TextRecognitionDataGenerator/run.py'\n",
    "TRAIN_SIZE = 8000\n",
    "TEST_SIZE = 2000\n",
    "\n",
    "# Original TRAINING images are named [INDEX].jpg\n",
    "subprocess.call(['python', \n",
    "                 DATAGEN_LOC, \n",
    "                 '--output_dir', \n",
    "                 TRAIN_LOC + '/original', \n",
    "                 '-na' , '2' ,\n",
    "                 '-l', 'sp', \n",
    "                 '-c', str(TRAIN_SIZE), \n",
    "                 '-w', '1']);\n",
    "\n",
    "# Original TESTING images are named [INDEX].jpg\n",
    "subprocess.call(['python', \n",
    "                 DATAGEN_LOC, \n",
    "                 '--output_dir', \n",
    "                 TEST_LOC + '/original', \n",
    "                 '-na' , '2' ,\n",
    "                 '-l', 'sp', \n",
    "                 '-c', str(TEST_SIZE), \n",
    "                 '-w', '1']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Augmentation\n",
    "__Now we will perform random augmentations on our original dataset and we will use this to feed into our denoising autoencoder - For every single original image, we will draw a random line in the image that may or may not cover the characters and a rotation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new directories for the augmented images\n",
    "mkdir(TRAIN_LOC + '/augmented')\n",
    "mkdir(TEST_LOC + '/augmented')\n",
    "\n",
    "# Training set\n",
    "for i in range(TRAIN_SIZE):\n",
    "    img = Image.open(TRAIN_LOC + '/original/%d.jpg' % i).rotate(np.random.randint(20))\n",
    "    w = random.randint(1, img.size[1]/2)\n",
    "    x1 = random.randint(0, img.size[0])\n",
    "    y1 = random.randint(0, img.size[1])\n",
    "    x2 = random.randint(0, img.size[0])\n",
    "    y2 = random.randint(0, img.size[1])\n",
    "    img_draw = ImageDraw.Draw(img)\n",
    "    img_draw.line([(x1, y1), (x2, y2)], width=w, fill='brown')\n",
    "    img.save(TRAIN_LOC + '/augmented/%d.jpg' % i)\n",
    "\n",
    "\n",
    "# Testing set\n",
    "for i in range(TEST_SIZE):\n",
    "    img = Image.open(TEST_LOC + '/original/%d.jpg' % i).rotate(np.random.randint(20))\n",
    "    w = random.randint(1, img.size[1]/2)\n",
    "    x1 = random.randint(0, img.size[0])\n",
    "    y1 = random.randint(0, img.size[1])\n",
    "    x2 = random.randint(0, img.size[0])\n",
    "    y2 = random.randint(0, img.size[1])\n",
    "    img_draw = ImageDraw.Draw(img)\n",
    "    img_draw.line([(x1, y1), (x2, y2)], width=w, fill='brown')\n",
    "    img.save(TEST_LOC + '/augmented/%d.jpg' % i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Pre-processing\n",
    "__Now that we have both the original unobstructed images and the augmented images, we can now take a look at the images as well as reshape them for our denoising auto encoder__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read an example Image from the training set with matplotlib\n",
    "import matplotlib.image as mpimg \n",
    "\n",
    "# Random idx for us to see the original and augmented image\n",
    "rand_idx = random.randint(0, TRAIN_SIZE)\n",
    "  \n",
    "# Output Images Settings\n",
    "rcParams['figure.figsize'] = 20,20\n",
    "fig, ax = plt.subplots(5,2)\n",
    "\n",
    "# Displaying a list of the originals and augmented side by side\n",
    "for row_idx in range(5):\n",
    "    \n",
    "    img_original = mpimg.imread(TRAIN_LOC + '/original/%d.jpg' % (rand_idx + row_idx)) \n",
    "    img_augmented = mpimg.imread(TRAIN_LOC + '/augmented/%d.jpg' % (rand_idx + row_idx)) \n",
    "\n",
    "    # Original Image\n",
    "    ax[row_idx, 0].set_title('Original Image (Used for Decoder Loss)')\n",
    "    ax[row_idx, 0].imshow(img_original)\n",
    "\n",
    "    # Augmented Image\n",
    "    ax[row_idx, 1].set_title('Augmented Image (Fed into Encoder as Input)')\n",
    "    ax[row_idx, 1].imshow(img_augmented)\n",
    "    \n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Since all the images are of different sizes, let's resize them__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Resize and save the images\n",
    "# X_train = \n",
    "# X_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Denoising Auto Encoder Model\n",
    "__We will build our denoising auto encoder now, we use this article as our [reference](https://medium.com/@connectwithghosh/denoising-images-using-an-autoencoder-using-tensorflow-in-python-1e2e62932837)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Number of nodes in each layer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encoder \n",
    "# num_nodes_input = 784\n",
    "# num_nodes_hidden_encoder = 32\n",
    "\n",
    "# # Decoder\n",
    "# num_nodes_hidden_decoder = 32\n",
    "# num_nodes_output  = 784"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Model Layers__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Input Layer: Taking in images of shape 784\n",
    "# input_layer = tf.placeholder('float', [None, num_nodes_input])\n",
    "\n",
    "# # Hidden Layer (Encoder): 784*32 weights and 32 biases\n",
    "# hidden_layer_encoder_vals = {\n",
    "#     'weights': tf.Variable(tf.random_normal([num_nodes_input, num_nodes_hidden_encoder])),\n",
    "#     'biases': tf.Variable(tf.random_normal([num_nodes_hidden_encoder]))  \n",
    "# }\n",
    "\n",
    "# # Hidden layer (Decoder): 32*32 weights and 32 biases\n",
    "# hidden_layer_decoder_vals = {\n",
    "#     'weights': tf.Variable(tf.random_normal([num_nodes_hidden_encoder, num_nodes_hidden_decoder])),\n",
    "#     'biases': tf.Variable(tf.random_normal([num_nodes_hidden_decoder]))  \n",
    "# }\n",
    "\n",
    "# # Output layer: 32*784 weights and 784 biases\n",
    "# output_layer_vals = {\n",
    "#     'weights':tf.Variable(tf.random_normal([num_nodes_hidden_decoder, num_nodes_output])), \n",
    "#     'biases':tf.Variable(tf.random_normal([num_nodes_output])) \n",
    "# }\n",
    "\n",
    "# # Sigmoid activation on input layer values \n",
    "# # and hidden layer of encoder\n",
    "# encoding_layer = tf.nn.sigmoid(\n",
    "#     tf.add(\n",
    "#         tf.matmul(\n",
    "#             input_layer,\n",
    "#             hidden_layer_encoder_vals['weights']\n",
    "#         ),\n",
    "#         hidden_layer_encoder_vals['biases']\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # Sigmoid activation on output of encoding layer \n",
    "# # and the hidden layer of decoder\n",
    "# decoding_layer = tf.nn.sigmoid(\n",
    "#     tf.add(\n",
    "#         tf.matmul(\n",
    "#             layer_1,\n",
    "#             hidden_2_layer_vals['weights']\n",
    "#         ),\n",
    "#         hidden_2_layer_vals['biases']\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # Multiply output of decoding layer with the output layer values\n",
    "# # with Weight and add Bias\n",
    "# output_layer = tf.matmul(\n",
    "#     decoding_layer,\n",
    "#     output_layer_vals['weights']\n",
    "# ) + output_layer_vals['biases']\n",
    "\n",
    "# # Output_true shall have the original image for error calculations\n",
    "# output_true = tf.placeholder('float', [None, 784])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Loss Function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defining our cost function as MSE\n",
    "# mse = tf.reduce_mean(tf.square(output_layer - output_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Optimizer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defining our optimizer: How fast the model should learn\n",
    "# learn_rate = 0.1\n",
    "# optimizer = tf.train.AdagradOptimizer(learn_rate).minimize(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create Session__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialising session\n",
    "# init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "    \n",
    "#     sess.run(init)\n",
    "    \n",
    "#     # Defining batch size, number of epochs and learning rate\n",
    "#     batch_size = 100  # how many images to use together for training\n",
    "#     hm_epochs =1000    # how many times to go through the entire dataset\n",
    "#     tot_images = X_train.shape[0] # total number of images\n",
    "\n",
    "#     # Running the model for a 10000 epochs taking 100 images in batches\n",
    "#     # total improvement is printed out after each epoch\n",
    "#     for epoch in range(hm_epochs):\n",
    "        \n",
    "#         # Initializing error as 0\n",
    "#         epoch_loss = 0 \n",
    "        \n",
    "#         for i in range(int(tot_images/batch_size)):\n",
    "#             epoch_x = X_train[i * batch_size : (i + 1) * batch_size]\n",
    "#             _, c = sess.run([optimizer, meansq],\\\n",
    "#                    feed_dict={input_layer: epoch_x, \\\n",
    "#                               output_true: epoch_x})\n",
    "#             epoch_loss += c\n",
    "            \n",
    "#         print('Epoch', epoch, '/', hm_epochs, 'loss:',epoch_loss)\n",
    "        \n",
    "#     # Test the Train Model\n",
    "# #     matches = \n",
    "    \n",
    "# #     accuracy = \n",
    "    \n",
    "# #     print(sess.run(accuracy, feed_dict={x: ,\n",
    "# #                                         y_true: }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pick any image\n",
    "# any_image = X_test_noisy[234]\n",
    "\n",
    "# # run it though the autoencoder\n",
    "# output_any_image = sess.run(output_layer,\\\n",
    "#                    feed_dict={input_layer:[any_image]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
