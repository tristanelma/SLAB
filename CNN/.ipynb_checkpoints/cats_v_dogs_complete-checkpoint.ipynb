{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cats vs Dogs\n",
    "For this workshop you will be building a Convolutional neural network to classify cats vs dogs. You will need to be familiar with the theory of CNNs. Visit our lesson [here](http://caisplusplus.usc.edu/blog/curriculum/lesson7) for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports, make sure you have cv2 installed!\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.ndimage import imread\n",
    "import cv2\n",
    "import sklearn.utils\n",
    "\n",
    "# You are not allowed to change any of these constants.\n",
    "\n",
    "INPUT_STRING = 'burrito'\n",
    "DATA_PATH_1 = 'data_generation/training/positive_samples/'\n",
    "DATA_PATH_2 = 'data_generation/training/false_samples/'\n",
    "TEST_PERCENT = 0.1\n",
    "SELECT_SUBSET_PERCENT = 0.15\n",
    "\n",
    "# The cat and dog images are of variable size.\n",
    "RESIZE_WIDTH=32\n",
    "RESIZE_HEIGHT=32\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to load 1200 files\n",
      "Have loaded 1000 samples\n",
      "Going to load 1200 files\n",
      "Have loaded 1000 samples\n",
      "Train set has dimensionality (2160, 32, 32, 3)\n",
      "Test set has dimensionality (240, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Lets get started by loading the data.\n",
    "# Make sure you have the data downloaded to ./data\n",
    "# To download the data go to https://www.kaggle.com/c/dogs-vs-cats/data and download train.zip\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "files_1 = os.listdir(DATA_PATH_1)\n",
    "# Shuffle so we are selecting about an equal number of dog and cat images.\n",
    "shuffled_files_1 = sklearn.utils.shuffle(files_1)\n",
    "select_count_1 = int(len(shuffled_files_1) * SELECT_SUBSET_PERCENT)\n",
    "\n",
    "print('Going to load %i files' % select_count_1)\n",
    "\n",
    "subset_files_select_1 = shuffled_files_1[:select_count_1]\n",
    "\n",
    "DISPLAY_COUNT = 1000\n",
    "\n",
    "for i, input_file in enumerate(subset_files_select_1):\n",
    "    if i % DISPLAY_COUNT == 0 and i != 0:\n",
    "        print('Have loaded %i samples' % i)\n",
    "        \n",
    "    img = imread(DATA_PATH_1 + input_file)\n",
    "    img = cv2.resize(img, (RESIZE_WIDTH, RESIZE_HEIGHT), interpolation=cv2.INTER_CUBIC)\n",
    "    X.append(img)\n",
    "    Y.append(0.0)\n",
    "    \n",
    "\n",
    "files_2 = os.listdir(DATA_PATH_2)\n",
    "# Shuffle so we are selecting about an equal number of dog and cat images.\n",
    "shuffled_files_2 = sklearn.utils.shuffle(files_2)\n",
    "select_count_2 = int(len(shuffled_files_2) * SELECT_SUBSET_PERCENT)\n",
    "\n",
    "print('Going to load %i files' % select_count_2)\n",
    "\n",
    "subset_files_select_2 = shuffled_files_2[:select_count_2]\n",
    "    \n",
    "for i, input_file in enumerate(subset_files_select_2):\n",
    "    if i % DISPLAY_COUNT == 0 and i != 0:\n",
    "        print('Have loaded %i samples' % i)\n",
    "        \n",
    "    img = imread(DATA_PATH_2 + input_file)\n",
    "    img = cv2.resize(img, (RESIZE_WIDTH, RESIZE_HEIGHT), interpolation=cv2.INTER_CUBIC)\n",
    "    X.append(img)\n",
    "    Y.append(1.0)\n",
    "        \n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "test_size = int(len(X) * TEST_PERCENT)\n",
    "\n",
    "test_X = X[:test_size]\n",
    "test_Y = Y[:test_size]\n",
    "train_X = X[test_size:]\n",
    "train_Y = Y[test_size:]\n",
    "\n",
    "print('Train set has dimensionality %s' % str(train_X.shape))\n",
    "print('Test set has dimensionality %s' % str(test_X.shape))\n",
    "\n",
    "# Apply some normalization here.\n",
    "train_X = train_X.astype('float32')\n",
    "test_X = test_X.astype('float32')\n",
    "train_X /= 255\n",
    "test_X /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "#TODO: (Optional)\n",
    "# Perform any data preprocessing steps\n",
    "\n",
    "\n",
    "\n",
    "######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the network\n",
    "Here are some useful resources to help with defining a powerful network.\n",
    "- Convolution layers (use the 2D convolution) https://keras.io/layers/convolutional/\n",
    "- Batch norm layer https://keras.io/layers/normalization/\n",
    "- Layer initializers https://keras.io/initializers/\n",
    "- Dense layer https://keras.io/layers/core/#dense\n",
    "- Activation functions https://keras.io/layers/core/#activation\n",
    "- Regulizers: \n",
    "    - https://keras.io/layers/core/#dropout\n",
    "    - https://keras.io/regularizers/\n",
    "    - https://keras.io/callbacks/#earlystopping\n",
    "    - https://keras.io/constraints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "#TODO:\n",
    "# Import necessary layers.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dropout, Flatten, Conv2D, MaxPooling2D, Dense, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.constraints import max_norm\n",
    "######################################\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "######################################\n",
    "#TODO:\n",
    "# Define the network\n",
    "model.add(Conv2D(32, (3, 3), padding='same', kernel_initializer='glorot_normal', input_shape=(RESIZE_WIDTH, RESIZE_HEIGHT, 3)))\n",
    "#model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding='same', kernel_initializer='glorot_normal'))\n",
    "#model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64,(3, 3), padding='same', kernel_initializer='glorot_normal'))\n",
    "#model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3), padding='same', kernel_initializer='glorot_normal'))\n",
    "#model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(512, kernel_initializer='glorot_normal'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, kernel_initializer='glorot_normal'))\n",
    "model.add(Activation('sigmoid'))\n",
    "######################################\n",
    "\n",
    "\n",
    "######################################\n",
    "#TODO:\n",
    "# Define your loss and your objective\n",
    "optimizer = RMSprop(lr=1e-4)\n",
    "loss = 'binary_crossentropy'\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Time\n",
    "Now it's time to actually test the network. Don't change any of the parameters here except for the batch size.\n",
    "\n",
    "Get above **65%**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1728 samples, validate on 432 samples\n",
      "Epoch 1/10\n",
      "1728/1728 [==============================] - 7s 4ms/step - loss: 0.6635 - acc: 0.5862 - val_loss: 0.6647 - val_acc: 0.8171\n",
      "Epoch 2/10\n",
      "1728/1728 [==============================] - 6s 3ms/step - loss: 0.6069 - acc: 0.6580 - val_loss: 0.6087 - val_acc: 0.8843\n",
      "Epoch 3/10\n",
      "1728/1728 [==============================] - 6s 4ms/step - loss: 0.5179 - acc: 0.7581 - val_loss: 0.4251 - val_acc: 0.9769\n",
      "Epoch 4/10\n",
      "1728/1728 [==============================] - 6s 4ms/step - loss: 0.4259 - acc: 0.8171 - val_loss: 0.3757 - val_acc: 0.9352\n",
      "Epoch 5/10\n",
      "1728/1728 [==============================] - 6s 4ms/step - loss: 0.3626 - acc: 0.8385 - val_loss: 0.1328 - val_acc: 0.9977\n",
      "Epoch 6/10\n",
      "1728/1728 [==============================] - 6s 3ms/step - loss: 0.3254 - acc: 0.8553 - val_loss: 0.3721 - val_acc: 0.8403\n",
      "Epoch 7/10\n",
      "1728/1728 [==============================] - 7s 4ms/step - loss: 0.2874 - acc: 0.8941 - val_loss: 0.3314 - val_acc: 0.8727\n",
      "Epoch 8/10\n",
      " 128/1728 [=>............................] - ETA: 5s - loss: 0.2885 - acc: 0.8750"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "#TODO:\n",
    "# Define the batch size\n",
    "batch_size = 64\n",
    "######################################\n",
    "model.fit(train_X, train_Y, batch_size=batch_size, epochs=EPOCHS, validation_split=0.2, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(test_X, test_Y, batch_size=batch_size, verbose=1)\n",
    "\n",
    "print('')\n",
    "print('Got %.2f%% accuracy' % (acc * 100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
