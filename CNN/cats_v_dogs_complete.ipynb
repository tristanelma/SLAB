{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cats vs Dogs\n",
    "For this workshop you will be building a Convolutional neural network to classify cats vs dogs. You will need to be familiar with the theory of CNNs. Visit our lesson [here](http://caisplusplus.usc.edu/blog/curriculum/lesson7) for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports, make sure you have cv2 installed!\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.ndimage import imread\n",
    "from scipy.misc import imsave\n",
    "import cv2\n",
    "import sklearn.utils\n",
    "import subprocess\n",
    "\n",
    "# You are not allowed to change any of these constants.\n",
    "\n",
    "INPUT_STRING = 'benbrooks'\n",
    "DATA_PATH_1 = 'data_generation/positive_samples/'\n",
    "DATA_PATH_2 = 'data_generation/false_samples/'\n",
    "DATA_PATH_3 = 'data_generation/'\n",
    "TEST_PERCENT = 0.2\n",
    "SELECT_SUBSET_PERCENT = 1\n",
    "\n",
    "# The cat and dog images are of variable size.\n",
    "RESIZE_WIDTH=32\n",
    "RESIZE_HEIGHT=32\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to load 20000 files\n",
      "Have loaded 1000 samples\n",
      "Have loaded 2000 samples\n",
      "Have loaded 3000 samples\n",
      "Have loaded 4000 samples\n",
      "Have loaded 5000 samples\n",
      "Have loaded 6000 samples\n",
      "Have loaded 7000 samples\n",
      "Have loaded 8000 samples\n",
      "Have loaded 9000 samples\n",
      "Have loaded 10000 samples\n",
      "Have loaded 11000 samples\n",
      "Have loaded 12000 samples\n",
      "Have loaded 13000 samples\n",
      "Have loaded 14000 samples\n",
      "Have loaded 15000 samples\n",
      "Have loaded 16000 samples\n",
      "Have loaded 17000 samples\n",
      "Have loaded 18000 samples\n",
      "Have loaded 19000 samples\n",
      "Train set has dimensionality (16000, 32, 32, 3)\n",
      "Test set has dimensionality (4000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Lets get started by loading the data.\n",
    "# Make sure you have the data downloaded to ./data\n",
    "# To download the data go to https://www.kaggle.com/c/dogs-vs-cats/data and download train.zip\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "DISPLAY_COUNT = 1000\n",
    "\n",
    "files_1 = os.listdir(DATA_PATH_1)\n",
    "for i, input_file in enumerate(files_1):\n",
    "    img = imread(DATA_PATH_1 + input_file)\n",
    "    img = cv2.resize(img, (RESIZE_WIDTH, RESIZE_HEIGHT), interpolation=cv2.INTER_CUBIC)\n",
    "    imsave('data_generation/t.' + str(i) + '.jpg', img)\n",
    "\n",
    "files_2 = os.listdir(DATA_PATH_2)\n",
    "for i, input_file in enumerate(files_2):\n",
    "    img = imread(DATA_PATH_2 + input_file)\n",
    "    img = cv2.resize(img, (RESIZE_WIDTH, RESIZE_HEIGHT), interpolation=cv2.INTER_CUBIC)\n",
    "    imsave('data_generation/f.' + str(i) + '.jpg', img)\n",
    "    \n",
    "subprocess.call(['rm', '-rf', 'data_generation/positive_samples'])\n",
    "subprocess.call(['rm', '-rf', 'data_generation/false_samples'])\n",
    "    \n",
    "files_3 = os.listdir(DATA_PATH_3)\n",
    "shuffled_files = sklearn.utils.shuffle(files_3)\n",
    "print('Going to load %i files' % len(shuffled_files))\n",
    "\n",
    "for i, input_file in enumerate(shuffled_files):\n",
    "    if i % DISPLAY_COUNT == 0 and i != 0:\n",
    "        print('Have loaded %i samples' % i)\n",
    "        \n",
    "    img = imread(DATA_PATH_3 + input_file)\n",
    "    img = cv2.resize(img, (RESIZE_WIDTH, RESIZE_HEIGHT), interpolation=cv2.INTER_CUBIC)\n",
    "    X.append(img)\n",
    "    if 't' == input_file.split('.')[0]:\n",
    "        Y.append(1.0)\n",
    "    else:\n",
    "        Y.append(0.0)\n",
    "        \n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "test_size = int(len(X) * TEST_PERCENT)\n",
    "\n",
    "test_X = X[:test_size]\n",
    "test_Y = Y[:test_size]\n",
    "train_X = X[test_size:]\n",
    "train_Y = Y[test_size:]\n",
    "\n",
    "print('Train set has dimensionality %s' % str(train_X.shape))\n",
    "print('Test set has dimensionality %s' % str(test_X.shape))\n",
    "\n",
    "# Apply some normalization here.\n",
    "train_X = train_X.astype('float32')\n",
    "test_X = test_X.astype('float32')\n",
    "train_X /= 255\n",
    "test_X /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "#TODO: (Optional)\n",
    "# Perform any data preprocessing steps\n",
    "\n",
    "\n",
    "\n",
    "######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the network\n",
    "Here are some useful resources to help with defining a powerful network.\n",
    "- Convolution layers (use the 2D convolution) https://keras.io/layers/convolutional/\n",
    "- Batch norm layer https://keras.io/layers/normalization/\n",
    "- Layer initializers https://keras.io/initializers/\n",
    "- Dense layer https://keras.io/layers/core/#dense\n",
    "- Activation functions https://keras.io/layers/core/#activation\n",
    "- Regulizers: \n",
    "    - https://keras.io/layers/core/#dropout\n",
    "    - https://keras.io/regularizers/\n",
    "    - https://keras.io/callbacks/#earlystopping\n",
    "    - https://keras.io/constraints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benbrooks/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "#TODO:\n",
    "# Import necessary layers.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dropout, Flatten, Conv2D, MaxPooling2D, Dense, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.constraints import max_norm\n",
    "######################################\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "######################################\n",
    "#TODO:\n",
    "# Define the network\n",
    "model.add(Conv2D(32, (3, 3), padding='same', kernel_initializer='glorot_normal', input_shape=(RESIZE_WIDTH, RESIZE_HEIGHT, 3)))\n",
    "#model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding='same', kernel_initializer='glorot_normal'))\n",
    "#model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64,(3, 3), padding='same', kernel_initializer='glorot_normal'))\n",
    "#model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3), padding='same', kernel_initializer='glorot_normal'))\n",
    "#model.add(BatchNormalization(axis=-1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(512, kernel_initializer='glorot_normal'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, kernel_initializer='glorot_normal'))\n",
    "model.add(Activation('sigmoid'))\n",
    "######################################\n",
    "\n",
    "\n",
    "######################################\n",
    "#TODO:\n",
    "# Define your loss and your objective\n",
    "optimizer = RMSprop(lr=1e-4)\n",
    "loss = 'binary_crossentropy'\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Time\n",
    "Now it's time to actually test the network. Don't change any of the parameters here except for the batch size.\n",
    "\n",
    "Get above **65%**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12800 samples, validate on 3200 samples\n",
      "Epoch 1/5\n",
      "12800/12800 [==============================] - 52s 4ms/step - loss: 0.4722 - acc: 0.7649 - val_loss: 0.3094 - val_acc: 0.8447\n",
      "Epoch 2/5\n",
      "12800/12800 [==============================] - 50s 4ms/step - loss: 0.2244 - acc: 0.9088 - val_loss: 0.1582 - val_acc: 0.9363\n",
      "Epoch 3/5\n",
      "12800/12800 [==============================] - 47s 4ms/step - loss: 0.1286 - acc: 0.9557 - val_loss: 0.1020 - val_acc: 0.9650\n",
      "Epoch 4/5\n",
      "12800/12800 [==============================] - 47s 4ms/step - loss: 0.0906 - acc: 0.9698 - val_loss: 0.0836 - val_acc: 0.9747\n",
      "Epoch 5/5\n",
      "12800/12800 [==============================] - 46s 4ms/step - loss: 0.0713 - acc: 0.9774 - val_loss: 0.0730 - val_acc: 0.9788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14742ceb8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################\n",
    "#TODO:\n",
    "# Define the batch size\n",
    "batch_size = 64\n",
    "######################################\n",
    "model.fit(train_X, train_Y, batch_size=batch_size, epochs=EPOCHS, validation_split=0.2, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 3s 800us/step\n",
      "\n",
      "Got 98.00% accuracy\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_X, test_Y, batch_size=batch_size, verbose=1)\n",
    "\n",
    "print('')\n",
    "print('Got %.2f%% accuracy' % (acc * 100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      0.97      0.98      1996\n",
      "        1.0       0.97      0.99      0.98      2004\n",
      "\n",
      "avg / total       0.98      0.98      0.98      4000\n",
      "\n",
      "[[1932   64]\n",
      " [  16 1988]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "predictions = model.predict(test_X)\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] < 0.5:\n",
    "        predictions[i] = 0.0\n",
    "    else:\n",
    "        predictions[i] = 1.0\n",
    "    \n",
    "print(classification_report(test_Y, predictions))\n",
    "print(confusion_matrix(test_Y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
